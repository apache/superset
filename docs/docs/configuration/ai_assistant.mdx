---
title: AI Assistant
hide_title: true
sidebar_position: 1
version: 1
---

# AI Assistant

The text-to-SQL AI translator (AI Assistant) supports users of [SQLLab](https://incubator-superset.readthedocs.io/en/latest/sqllab.html) with writing SQL queries to connected databases. [Preset](https://preset.io/), the enterprise version of Superset, also offers a similar [AI Assist feature](https://docs.preset.io/docs/ai-assisted-sql-querying).

In a nutshell:
In Superset SQLLab a text bar positioned above the query editor can be used to type a natural language question and generate SQL using a configurable LLM provider. Superset automatically formulates an LLM prompt that comprises:
- configurable database information (tables, columns, descriptions, indexes, foreign keys, common text column values, etc.)
- customizable system instructions
- the content of the SQL query editor
- the user question
The prompt is sent to the [LLM API](#llm_api), the SQL query is extracted from the response, validated, and appended to the SQL query editor along with the user's question.
Configuration of the AI Assistant, which allows the user to set the LLM API provider and adjust how the LLM context is generated, is done within the database connection configuration window, distinctly for each database connection.

## Using the AI Assistant

The AI Assistant text bar becomes visible in SQLLab only if the selected database connection has been configured to support the AI Assistant. Users can select a database from the dedicated dropdown menu, and if the AI Assistant is enabled for that connection, the text bar will appear, allowing them to interact with the feature. 

![assistant](../../static/img/screenshots/ai_assistant/assistant.png)

When the user types a question into the AI Assistant text bar and clicks **Generate SQL**, the AI Assistant constructs a prompt tailored for the configured [LLM API](#llm-api) associated with the selected database connection. It then interacts with the LLM API to generate a SQL query that translates the user's natural language question.

![asking_question](../../static/img/screenshots/ai_assistant/asking_question.png)

### Translation Mechanics

To ensure the AI Assistant generates precise SQL translations, the following steps are executed:

1. **Database Metadata**:  
The AI Assistant relies on comprehensive [database metadata](#database_metadata_data_pipeline) to construct accurate prompts for the LLM. This metadata is either cached by the LLM provider or directly included in the prompt, depending on the LLM provider's capabilities. 
   - If the configured LLM provider for the selected database supports explicit context caching (e.g., [Gemini](#cache)), the AI Assistant utilizes an LLM API session with cached database metadata. This approach eliminates the need to include the metadata directly in the prompt, thereby reducing the payload size of the API request, minimizing input token usage, and lowering associated costs.  
   - For providers that lack explicit context caching support, the database metadata is directly prepended to the LLM prompt, which may increase token usage and costs.

2. **Optional Schema Selection**:  
   - When schemas are selected using the dedicated dropdown menu, only the metadata for the selected schemas is included in the prompt. This metadata is directly prepended to the prompt without utilizing context caching functionality.  
   - An informational message displayed below the AI Assistant text bar informs users about this behavior.  
   - While this feature may enhance translation accuracy due to smaller context, it can also increase token usage and associated costs.  
   ![schema_selection](../../static/img/screenshots/ai_assistant/schema_selection.png) 

3. **Prompt Construction**:  
   - Includes system instructions configured for the LLM. If an LLM context cache is available, these instructions are stored within the cache; otherwise, they are directly appended to the prompt following the database metadata.
   - Appends the content of the SQLLab query editor to the prompt. By including the SQL query editor's content, users can build upon or refine their previous questions, fostering a conversational workflow akin to interacting with a chatbot.
   - Includes the user's question from the AI Assistant text bar.  

4. **LLM Interaction**:  
   - Sends the constructed prompt to the [LLM API](#llm_api) for processing.  
   - Extracts the SQL query from the LLM response.  

5. **Error Handling**:  
   - If SQL extraction fails, the error message is appended to the prompt and resubmitted to the LLM API for correction.  
   - This retry mechanism is executed twice before displaying an error message to the user.  
    ![failing_generate](../../static/img/screenshots/ai_assistant/failing_generate.png) 

6. **SQL Validation**:  
   - Executes an `EXPLAIN` statement to verify the syntax of the generated SQL query.  
   - If errors are detected, the retry mechanism is applied as described above.  

7. **Query Editor Update**:  
   - Comments out the existing content in the SQL query editor.  
   - Appends the user's question and the validated SQL query as a new entry.  
   ![successful_generation](../../static/img/screenshots/ai_assistant/successful_generation.png) 
   ![follow_up_question](../../static/img/screenshots/ai_assistant/follow_up_question.png) 

These steps ensure the AI Assistant delivers accurate and reliable SQL translations while maintaining a seamless user experience.

## Configuration

Configuration of the AI Assistant is managed within the database connection configuration panel, independently for each connection. As a result, the same permission rules that govern editing database connections (e.g., Admin role) apply.

### Supported Databases

The availability of the AI Assistant for supported [databases](databases.mdx) is primarily determined by the capability to construct the [database metadata context](#database_metadata_data_pipeline). If a specific configuration option is unavailable or the database type is unsupported, this is clearly indicated within the configuration panel. 

The AI Assistant currently supports the following databases:
- **PostgreSQL**: Full support  
- **SQLite**: Full support  
- **MySQL**: Full support  
- **TODO**: Additional databases to be added in future updates  

The **AI Assistant** panel in the **Edit database** modal provides administrators with all necessary configuration options. This panel includes:
- A toggle to enable or disable the AI Assistant for the selected database connection.
- Fields to configure the LLM API provider, API key, and model selection.
- Context settings for metadata generation, including periodicity, schema/table selection, and additional metadata options.
- A button to regenerate the database context on demand.

![configuration](../../static/img/screenshots/ai_assistant/configuration.png) 

This streamlined interface ensures administrators can efficiently manage AI Assistant settings for each database connection.

Below are two sections:

### Language Models

This is where the administrator configures the LLM API provider, the API key, and selects the one of the models the provider makes available via its API.  
Supported providers:  
   - Gemini  
   - OpenAI (WIP)  
   - Anthropic (WIP) 

![llm_config](../../static/img/screenshots/ai_assistant/llm_config.png)  

### Context Settings

Contains configuration parameters for managing the periodic creation of the database metadata, which is locally cached in Redis as a JSON file. The available settings include:

- **Estimated Context Token Count**: Provides an estimate of the token count of the generated context.
- **Last Context Build Timestamp**: Displays the timestamp of the most recent context creation.
- **Automatic Context Creatin Periodicity**: Specifies the interval (in hours) for automatic context creation executed by a Celery worker (default: 12 hours).
- **Schema and Table/View Selection**: Allows users to select the specific schemas and tables/views to include in metadata retrieval using checkboxes.
- **Include Index Information**: Option to include details about column indexes in the metadata.
- **Top-K Most Common Column Values**: Configures the number of most common values (*top_k*, default: 10) to retrieve from the last *n* records (default: 50,000) for text columns.
- **LLM System Instructions**: Provides a text editor to define system instructions for the LLM, with a sensible default value.
- **Regenerate Context Button**: Enables users to manually trigger context regeneration on demand.

![context_refresh](../../static/img/screenshots/ai_assistant/context_refresh.png) 
![table_selection](../../static/img/screenshots/ai_assistant/table_selection.png) 
![system_instructions](../../static/img/screenshots/ai_assistant/system_instructions.png) 

These settings ensure flexibility and control over the metadata generation process, optimizing the context for LLM interactions.
In the database connection summary table, there’s a column showing if the AI Assistant is enabled and correctly configured.

![summary_connections](../../static/img/screenshots/ai_assistant/summary_connections.png) 

<a name="database_metadata_data_pipeline"></a>

## Database Metadata Data Pipeline

Database metadata context is periodically regenerated by a data pipeline executed by a Celery worker, which retrieves the database metadata based on the AI Assistant configuration for the specific database connection. The resulting metadata is compiled into a compact nested JSON file and stored in the Redis cache for efficient access. 

The data pipeline utilizes the database metadata that Superset already collects and displays in SQLLab. This is the same  that metadata becomes available when users select tables using the dedicated dropdown menu. Once selected, the relevant information is displayed in the left sidebar and within the information tabs located beneath the SQL query editor. 

This comprehensive metadata ensures the LLM has the necessary context to generate accurate and efficient SQL queries. It constist of:
- **Schemas**: Schemas is the database.
   - **Description**: SQL comments describing the schema.
   - **Relations**: Tables and views within the schema.  
      - **Type**: Specifies whether the relation is a table or a view.  
      - **Description**: SQL comments describing the relation.  
      - **Columns**: Detailed information about each column in the relation.  
         - **Data Type**: The column's data type.  
         - **Nullable**: Indicates whether the column allows null values.  
         - **Description**: SQL comments describing the column.  
         - **Most Common Values**: The top-K most frequent values for text columns, derived from the last N records (default: 10 values from 50,000 records). This helps the LLM accurately filter columns based on approximate user input.  
      - **Indexes**: Information about indexes on the relation.  
         - **Uniqueness**: Specifies if the index enforces uniqueness.  
         - **Indexed Columns**: Lists the columns included in the index.  
         - **SQL Definition**: The SQL statement defining the index.  
      - **Foreign Keys**: Details about foreign key constraints.  
         - Helps construct accurate JOIN clauses by identifying relationships between tables.  

<a name="llm_api"></a>

## LLM API

### The Dispatcher

Calls to the LLMs are routed through the LLM dispatcher, which serves as the central interface for managing interactions with various LLM APIs. The dispatcher provides the following functionalities:

1. **Context Builder**:  
   Triggers a Celery worker to execute the database metadata data pipeline and generate a JSON file.

2. **LLM State Validation**:  
   Verifies whether Superset is configured to call an LLM API for the selected database connection.

3. **SQL Generation**:  
   Routes SQL generation requests to the appropriate LLM API based on the selected database and configuration.

4. **Default Settings Interface**:  
   Supplies default configuration values for the settings page, tailored to the selected LLM and database type. Additionally, it provides a list of available models for each LLM.

These interfaces ensure seamless integration between Superset and the configured LLMs, enabling efficient and accurate SQL generation.
   The dispatcher leverages the following inputs to construct prompts for SQL generation:

   1. The selected database connection.  
   2. The schemas chosen via the dropdown menu.  
   3. The content of the SQL query editor.  
   4. The user's input from the AI Assistant text bar.

   The dispatcher adheres to a singleton pattern. When a SQL generation request is initiated, it ensures the appropriate LLM API wrapper class is initialized with the relevant database context. This initialization occurs either when the wrapper has not been previously set up or when the database context has been recently regenerated, rendering the current wrapper outdated.

   If specific schemas are selected in the dropdown menu, the dispatcher includes metadata for only those schemas directly in the prompt. In such cases, it bypasses the context caching functionality provided by the LLM API, even if the provider supports it.  

### Gemini

There are at least three SDKs that can be used to interact with at least two APIs:

1. [Gemini Developer API](https://ai.google.dev/gemini-api/docs)  
2. [Vertex AI API](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference)

The Python SDKs are: 

3. One python SDK is [google-generativeai](https://pypi.org/project/google-generativeai/) which supports the Gemini Developer API.  
4. Another SDK is [google-genai](https://pypi.org/project/google-genai/) which supports both API, Gemini Developer API and Vertex AI API.  
5. Finally [google-cloud-aiplatform](https://pypi.org/project/google-cloud-aiplatform/) which supports Vertex AI API. 

We created a billable [Gemini API key via our GCP project](https://console.cloud.google.com/apis/api/generativelanguage.googleapis.com/metrics?project=data-prod-448623). This API key can be used only with the Gemini Developer API. The SDK that we manage to make work is the google-genai python package. It allows us to configure the API key for the LLM client, create multiple caches, each containing a list of content (such as our JSON metadata file), system instructions and cache configuration and then to interrogate the LLM client using one of those caches as context.

#### Cache

For the Gemini API specifically, we want to make use of Gemini’s caching APIs to improve speed and reduce costs. The Gemini LLM API wrapper takes responsibility for managing the cache. When the first request after initialization comes through, the wrapper will create a cache with the database context.  
Gemini offers a [caching functionality](https://ai.google.dev/gemini-api/docs/caching?lang=python) that can persist long context for a configurable amount of time, so that it needs to be processed once rather than for every LLM query, significantly reducing input token processing cost.   
Specifically one can configure the cache to hold system instructions, files, and time to live (TTL) or expiration time. The user of the Gemini API has to take responsibility for managing the cache and checking if the cache is still valid, and re-uploading the documents if it has expired. 

### OpenAI

…

#### Cache

OpenAI doesn’t offer an API for explicit control over the cache, and the full database context needs to be sent with every call to the completions API. However, OpenAI does maintain an [automatic cache](https://platform.openai.com/docs/guides/prompt-caching) that performs text matching on an incoming API request and compares it to previous requests. Due to that behaviour, the context and system prompt should always appear at the beginning of the input prompt.

### Anthropic

…

#### Cache

Caching with the Anthropic API works by prefix matching as with OpenAI, but has to be explicitly [triggered by inserting cache control blocks](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching) as part of the system prompt. As with the Gemini and OpenAI LLM implementations, the Anthropic module should take responsibility for controlling the cache.
