# Apache Superset 查询引擎

## 1. 查询引擎概述

### 1.1 查询引擎架构

```
查询引擎架构
├── 查询构建 (Query Building)
│   ├── QueryObject - 查询对象
│   ├── QueryBuilder - 查询构建器
│   └── SQLGenerator - SQL 生成器
├── 查询执行 (Query Execution)
│   ├── QueryExecutor - 查询执行器
│   ├── DatabaseConnector - 数据库连接器
│   └── ResultProcessor - 结果处理器
├── 查询优化 (Query Optimization)
│   ├── QueryOptimizer - 查询优化器
│   ├── CacheManager - 缓存管理器
│   └── QueryAnalyzer - 查询分析器
└── 查询监控 (Query Monitoring)
    ├── QueryLogger - 查询日志
    ├── PerformanceMonitor - 性能监控
    └── ErrorTracker - 错误跟踪
```

## 2. 查询构建

### 2.1 查询对象

[common/query_object.py](../superset/common/query_object.py) 定义了查询对象：

```python
from typing import Any, Optional, List
from dataclasses import dataclass

@dataclass
class QueryObject:
    """查询对象"""

    metrics: list[Any] | None = None
    groupby: list[Any] | None = None
    columns: list[Any] | None = None
    granularity: Optional[str] = None
    from_dttm: Optional[str] = None
    to_dttm: Optional[str] = None
    filters: list[Any] | None = None
    orderby: list[Any] | None = None
    row_limit: Optional[int] = None
    extras: dict[str, Any] | None = None
    is_timeseries: bool = False
    timeseries_limit: Optional[int] = None
    timeseries_limit_metric: Optional[Any] = None
    order_desc: bool = True
    post_processing: list[Any] | None = None
    custom_sql: Optional[str] = None
    custom_form_data: Optional[dict[str, Any]] = None

    def __post_init__(self):
        if self.metrics is None:
            self.metrics = []
        if self.groupby is None:
            self.groupby = []
        if self.columns is None:
            self.columns = []
        if self.filters is None:
            self.filters = []
        if self.orderby is None:
            self.orderby = []
        if self.extras is None:
            self.extras = {}
        if self.post_processing is None:
            self.post_processing = []

    def to_dict(self) -> dict[str, Any]:
        """转换为字典"""
        return {
            'metrics': self.metrics,
            'groupby': self.groupby,
            'columns': self.columns,
            'granularity': self.granularity,
            'from_dttm': self.from_dttm,
            'to_dttm': self.to_dttm,
            'filters': self.filters,
            'orderby': self.orderby,
            'row_limit': self.row_limit,
            'extras': self.extras,
            'is_timeseries': self.is_timeseries,
            'timeseries_limit': self.timeseries_limit,
            'timeseries_limit_metric': self.timeseries_limit_metric,
            'order_desc': self.order_desc,
            'post_processing': self.post_processing,
            'custom_sql': self.custom_sql,
            'custom_form_data': self.custom_form_data,
        }

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> 'QueryObject':
        """从字典创建"""
        return cls(**data)
```

### 2.2 查询构建器

```python
from typing import Any, Optional, List
from superset.common.query_object import QueryObject
from superset.models.core import SqlaTable

class QueryBuilder:
    """查询构建器"""

    def __init__(self, datasource: SqlaTable):
        self.datasource = datasource
        self.query_object = QueryObject()

    def set_metrics(self, metrics: List[Any]) -> 'QueryBuilder':
        """设置指标"""
        self.query_object.metrics = metrics
        return self

    def set_groupby(self, groupby: List[Any]) -> 'QueryBuilder':
        """设置分组"""
        self.query_object.groupby = groupby
        return self

    def set_columns(self, columns: List[Any]) -> 'QueryBuilder':
        """设置列"""
        self.query_object.columns = columns
        return self

    def set_granularity(self, granularity: str) -> 'QueryBuilder':
        """设置粒度"""
        self.query_object.granularity = granularity
        return self

    def set_time_range(self, from_dttm: str, to_dttm: str) -> 'QueryBuilder':
        """设置时间范围"""
        self.query_object.from_dttm = from_dttm
        self.query_object.to_dttm = to_dttm
        return self

    def set_filters(self, filters: List[Any]) -> 'QueryBuilder':
        """设置过滤器"""
        self.query_object.filters = filters
        return self

    def set_orderby(self, orderby: List[Any]) -> 'QueryBuilder':
        """设置排序"""
        self.query_object.orderby = orderby
        return self

    def set_row_limit(self, row_limit: int) -> 'QueryBuilder':
        """设置行限制"""
        self.query_object.row_limit = row_limit
        return self

    def build(self) -> QueryObject:
        """构建查询对象"""
        return self.query_object
```

### 2.3 SQL 生成器

```python
from typing import Any, Optional
from superset.common.query_object import QueryObject
from superset.models.core import SqlaTable
from sqlalchemy import select, func, and_, or_
from sqlalchemy.sql import Column

class SQLGenerator:
    """SQL 生成器"""

    def __init__(self, datasource: SqlaTable):
        self.datasource = datasource
        self.table = datasource.get_sqla_table()

    def generate_sql(self, query_object: QueryObject) -> str:
        """生成 SQL"""
        # 构建基础查询
        query = select()

        # 添加列
        if query_object.columns:
            for col_name in query_object.columns:
                query = query.add_columns(self.table.c[col_name])

        # 添加指标
        if query_object.metrics:
            for metric in query_object.metrics:
                if hasattr(metric, 'get_sqla_expression'):
                    query = query.add_columns(metric.get_sqla_expression())
                else:
                    query = query.add_columns(func.sum(self.table.c[metric]))

        # 添加分组
        if query_object.groupby:
            for group in query_object.groupby:
                query = query.group_by(self.table.c[group])

        # 添加时间范围
        if query_object.from_dttm and query_object.to_dttm:
            time_col = self.datasource.main_dttm_col
            query = query.where(
                and_(
                    self.table.c[time_col] >= query_object.from_dttm,
                    self.table.c[time_col] <= query_object.to_dttm,
                )
            )

        # 添加过滤器
        if query_object.filters:
            filter_conditions = []
            for filter_ in query_object.filters:
                filter_conditions.append(self._build_filter(filter_))
            query = query.where(and_(*filter_conditions))

        # 添加排序
        if query_object.orderby:
            for order in query_object.orderby:
                if query_object.order_desc:
                    query = query.order_by(self.table.c[order].desc())
                else:
                    query = query.order_by(self.table.c[order].asc())

        # 添加限制
        if query_object.row_limit:
            query = query.limit(query_object.row_limit)

        return str(query)

    def _build_filter(self, filter_: Any) -> Any:
        """构建过滤器"""
        from superset.filters import BasicFilter, AdhocFilter

        if isinstance(filter_, BasicFilter):
            return self._build_basic_filter(filter_)
        elif isinstance(filter_, AdhocFilter):
            return self._build_adhoc_filter(filter_)
        else:
            raise ValueError(f"不支持的过滤器类型: {type(filter_)}")

    def _build_basic_filter(self, filter_: Any) -> Any:
        """构建基础过滤器"""
        col = self.table.c[filter_.column]

        if filter_.operator == '==':
            return col == filter_.value
        elif filter_.operator == '!=':
            return col != filter_.value
        elif filter_.operator == '>':
            return col > filter_.value
        elif filter_.operator == '<':
            return col < filter_.value
        elif filter_.operator == '>=':
            return col >= filter_.value
        elif filter_.operator == '<=':
            return col <= filter_.value
        elif filter_.operator == 'in':
            return col.in_(filter_.value)
        elif filter_.operator == 'not in':
            return col.notin_(filter_.value)
        elif filter_.operator == 'like':
            return col.like(filter_.value)
        elif filter_.operator == 'ilike':
            return col.ilike(filter_.value)
        else:
            raise ValueError(f"不支持的运算符: {filter_.operator}")

    def _build_adhoc_filter(self, filter_: Any) -> Any:
        """构建 Adhoc 过滤器"""
        from sqlalchemy.sql import text

        return text(filter_.sqlExpression)
```

## 3. 查询执行

### 3.1 查询执行器

```python
from typing import Any, Optional
import pandas as pd
from superset.models.core import Database
from superset.common.query_object import QueryObject

class QueryExecutor:
    """查询执行器"""

    def __init__(self, database: Database):
        self.database = database
        self.engine = database.get_sqla_engine()

    def execute(self, sql: str) -> pd.DataFrame:
        """执行 SQL 查询"""
        with self.engine.connect() as connection:
            df = pd.read_sql_query(sql, connection)
        return df

    def execute_query_object(self, query_object: QueryObject, datasource: Any) -> pd.DataFrame:
        """执行查询对象"""
        # 生成 SQL
        from superset.common.query_processor import SQLGenerator
        sql_generator = SQLGenerator(datasource)
        sql = sql_generator.generate_sql(query_object)

        # 执行 SQL
        df = self.execute(sql)

        return df

    def execute_with_retry(self, sql: str, max_retries: int = 3) -> pd.DataFrame:
        """带重试的执行"""
        import time

        for attempt in range(max_retries):
            try:
                return self.execute(sql)
            except Exception as e:
                if attempt == max_retries - 1:
                    raise
                time.sleep(2 ** attempt)  # 指数退避
```

### 3.2 数据库连接器

```python
from typing import Any, Optional
from sqlalchemy import create_engine, Engine
from sqlalchemy.pool import QueuePool
from superset.models.core import Database

class DatabaseConnector:
    """数据库连接器"""

    def __init__(self, database: Database):
        self.database = database
        self._engine: Optional[Engine] = None

    def get_engine(self) -> Engine:
        """获取数据库引擎"""
        if self._engine is None:
            self._engine = self._create_engine()
        return self._engine

    def _create_engine(self) -> Engine:
        """创建数据库引擎"""
        return create_engine(
            self.database.sqlalchemy_uri,
            poolclass=QueuePool,
            pool_size=5,
            max_overflow=10,
            pool_pre_ping=True,
            pool_recycle=3600,
        )

    def test_connection(self) -> bool:
        """测试连接"""
        try:
            engine = self.get_engine()
            with engine.connect() as connection:
                connection.execute("SELECT 1")
            return True
        except Exception:
            return False

    def close(self) -> None:
        """关闭连接"""
        if self._engine:
            self._engine.dispose()
            self._engine = None
```

### 3.3 结果处理器

```python
from typing import Any, List
import pandas as pd
from superset.utils.database import GenericDataType

class ResultProcessor:
    """结果处理器"""

    def __init__(self, df: pd.DataFrame):
        self.df = df

    def process(self) -> dict[str, Any]:
        """处理结果"""
        return {
            'data': self._convert_to_records(),
            'coltypes': self._get_coltypes(),
            'rowcount': len(self.df),
            'columns': list(self.df.columns),
        }

    def _convert_to_records(self) -> List[dict[str, Any]]:
        """转换为记录"""
        return self.df.to_dict('records')

    def _get_coltypes(self) -> List[GenericDataType]:
        """获取列类型"""
        coltypes = []

        for col in self.df.columns:
            dtype = self.df[col].dtype

            if pd.api.types.is_integer_dtype(dtype):
                coltypes.append(GenericDataType.NUMERIC)
            elif pd.api.types.is_float_dtype(dtype):
                coltypes.append(GenericDataType.NUMERIC)
            elif pd.api.types.is_datetime64_any_dtype(dtype):
                coltypes.append(GenericDataType.TEMPORAL)
            elif pd.api.types.is_bool_dtype(dtype):
                coltypes.append(GenericDataType.BOOLEAN)
            else:
                coltypes.append(GenericDataType.STRING)

        return coltypes

    def apply_post_processing(self, post_processing: List[Any]) -> pd.DataFrame:
        """应用后处理"""
        for processor in post_processing:
            if processor['operation'] == 'pivot':
                self.df = self._pivot(processor)
            elif processor['operation'] == 'rolling':
                self.df = self._rolling(processor)
            elif processor['operation'] == 'cumulative':
                self.df = self._cumulative(processor)

        return self.df

    def _pivot(self, processor: dict[str, Any]) -> pd.DataFrame:
        """透视"""
        index = processor['index']
        columns = processor['columns']
        values = processor['values']

        return self.df.pivot_table(
            index=index,
            columns=columns,
            values=values,
            aggfunc='sum',
        ).reset_index()

    def _rolling(self, processor: dict[str, Any]) -> pd.DataFrame:
        """滚动计算"""
        column = processor['column']
        window = processor['window']
        operation = processor['operation']

        if operation == 'mean':
            self.df[f'{column}_rolling'] = self.df[column].rolling(window=window).mean()
        elif operation == 'sum':
            self.df[f'{column}_rolling'] = self.df[column].rolling(window=window).sum()
        elif operation == 'std':
            self.df[f'{column}_rolling'] = self.df[column].rolling(window=window).std()

        return self.df

    def _cumulative(self, processor: dict[str, Any]) -> pd.DataFrame:
        """累计计算"""
        column = processor['column']
        operation = processor['operation']

        if operation == 'sum':
            self.df[f'{column}_cumsum'] = self.df[column].cumsum()
        elif operation == 'mean':
            self.df[f'{column}_cummean'] = self.df[column].expanding().mean()

        return self.df
```

## 4. 查询优化

### 4.1 查询优化器

```python
from typing import Any, Optional
from superset.common.query_object import QueryObject

class QueryOptimizer:
    """查询优化器"""

    def __init__(self, datasource: Any):
        self.datasource = datasource

    def optimize(self, query_object: QueryObject) -> QueryObject:
        """优化查询"""
        # 优化指标
        query_object = self._optimize_metrics(query_object)

        # 优化分组
        query_object = self._optimize_groupby(query_object)

        # 优化过滤器
        query_object = self._optimize_filters(query_object)

        # 优化排序
        query_object = self._optimize_orderby(query_object)

        # 优化限制
        query_object = self._optimize_row_limit(query_object)

        return query_object

    def _optimize_metrics(self, query_object: QueryObject) -> QueryObject:
        """优化指标"""
        # 移除重复的指标
        seen = set()
        unique_metrics = []
        for metric in query_object.metrics:
            metric_key = str(metric)
            if metric_key not in seen:
                seen.add(metric_key)
                unique_metrics.append(metric)

        query_object.metrics = unique_metrics

        return query_object

    def _optimize_groupby(self, query_object: QueryObject) -> QueryObject:
        """优化分组"""
        # 移除重复的分组
        seen = set()
        unique_groupby = []
        for group in query_object.groupby:
            if group not in seen:
                seen.add(group)
                unique_groupby.append(group)

        query_object.groupby = unique_groupby

        return query_object

    def _optimize_filters(self, query_object: QueryObject) -> QueryObject:
        """优化过滤器"""
        # 合并相同的过滤器
        from collections import defaultdict

        filter_map = defaultdict(list)
        for filter_ in query_object.filters:
            if hasattr(filter_, 'column'):
                filter_map[filter_.column].append(filter_)

        optimized_filters = []
        for column, filters in filter_map.items():
            if len(filters) == 1:
                optimized_filters.extend(filters)
            else:
                # 尝试合并过滤器
                merged_filter = self._merge_filters(filters)
                if merged_filter:
                    optimized_filters.append(merged_filter)
                else:
                    optimized_filters.extend(filters)

        query_object.filters = optimized_filters

        return query_object

    def _merge_filters(self, filters: List[Any]) -> Optional[Any]:
        """合并过滤器"""
        # 简单实现：合并 IN 操作
        in_filters = [f for f in filters if f.operator == 'in']
        if len(in_filters) > 1:
            values = set()
            for f in in_filters:
                values.update(f.value)
            return in_filters[0].__class__(
                column=in_filters[0].column,
                operator='in',
                value=list(values),
            )
        return None

    def _optimize_orderby(self, query_object: QueryObject) -> QueryObject:
        """优化排序"""
        # 移除重复的排序
        seen = set()
        unique_orderby = []
        for order in query_object.orderby:
            if order not in seen:
                seen.add(order)
                unique_orderby.append(order)

        query_object.orderby = unique_orderby

        return query_object

    def _optimize_row_limit(self, query_object: QueryObject) -> QueryObject:
        """优化行限制"""
        # 设置合理的默认限制
        if query_object.row_limit is None or query_object.row_limit > 100000:
            query_object.row_limit = 100000

        return query_object
```

### 4.2 缓存管理器

```python
from typing import Any, Optional
import hashlib
import json
from superset.extensions import cache_manager

class QueryCacheManager:
    """查询缓存管理器"""

    def __init__(self):
        self.cache = cache_manager

    def get_cache_key(self, query_object: Any, datasource: Any) -> str:
        """获取缓存键"""
        cache_data = {
            'datasource_id': datasource.id,
            'query': query_object.to_dict() if hasattr(query_object, 'to_dict') else query_object,
        }

        cache_string = json.dumps(cache_data, sort_keys=True)
        cache_key = hashlib.md5(cache_string.encode()).hexdigest()

        return f"query:{cache_key}"

    def get_cached_result(self, query_object: Any, datasource: Any) -> Optional[dict[str, Any]]:
        """获取缓存结果"""
        cache_key = self.get_cache_key(query_object, datasource)
        return self.cache.get(cache_key)

    def cache_result(self, query_object: Any, datasource: Any, result: dict[str, Any], timeout: int = 3600) -> None:
        """缓存结果"""
        cache_key = self.get_cache_key(query_object, datasource)
        self.cache.set(cache_key, result, timeout=timeout)

    def invalidate_cache(self, query_object: Any, datasource: Any) -> None:
        """使缓存失效"""
        cache_key = self.get_cache_key(query_object, datasource)
        self.cache.delete(cache_key)

    def clear_all_cache(self) -> None:
        """清空所有缓存"""
        self.cache.clear()
```

### 4.3 查询分析器

```python
from typing import Any, Optional
from superset.common.query_object import QueryObject

class QueryAnalyzer:
    """查询分析器"""

    def __init__(self, datasource: Any):
        self.datasource = datasource

    def analyze(self, query_object: QueryObject) -> dict[str, Any]:
        """分析查询"""
        return {
            'complexity': self._calculate_complexity(query_object),
            'estimated_rows': self._estimate_rows(query_object),
            'estimated_time': self._estimate_time(query_object),
            'recommendations': self._get_recommendations(query_object),
        }

    def _calculate_complexity(self, query_object: QueryObject) -> int:
        """计算查询复杂度"""
        complexity = 0

        # 指标复杂度
        complexity += len(query_object.metrics) * 2

        # 分组复杂度
        complexity += len(query_object.groupby) * 3

        # 过滤器复杂度
        complexity += len(query_object.filters) * 2

        # 排序复杂度
        complexity += len(query_object.orderby) * 2

        # 时间范围复杂度
        if query_object.from_dttm and query_object.to_dttm:
            complexity += 5

        return complexity

    def _estimate_rows(self, query_object: QueryObject) -> Optional[int]:
        """估计返回行数"""
        # 简单实现：使用 row_limit
        return query_object.row_limit

    def _estimate_time(self, query_object: QueryObject) -> Optional[float]:
        """估计执行时间（秒）"""
        complexity = self._calculate_complexity(query_object)

        # 简单实现：基于复杂度估计
        return complexity * 0.1

    def _get_recommendations(self, query_object: QueryObject) -> List[str]:
        """获取优化建议"""
        recommendations = []

        # 检查是否需要添加索引
        if len(query_object.groupby) > 3:
            recommendations.append("考虑减少分组字段数量")

        # 检查是否需要限制结果
        if query_object.row_limit is None or query_object.row_limit > 10000:
            recommendations.append("考虑添加行限制以提高性能")

        # 检查是否需要使用缓存
        if len(query_object.filters) == 0:
            recommendations.append("考虑使用缓存以提高重复查询性能")

        return recommendations
```

## 5. 查询监控

### 5.1 查询日志

```python
from typing import Any, Optional
import logging
from datetime import datetime

logger = logging.getLogger(__name__)

class QueryLogger:
    """查询日志"""

    def __init__(self):
        self.logger = logger

    def log_query(self, query: str, execution_time: float, row_count: int) -> None:
        """记录查询"""
        self.logger.info(
            f"Query executed: {query[:100]}... "
            f"Time: {execution_time:.2f}s "
            f"Rows: {row_count}"
        )

    def log_error(self, query: str, error: Exception) -> None:
        """记录错误"""
        self.logger.error(
            f"Query failed: {query[:100]}... "
            f"Error: {str(error)}"
        )

    def log_slow_query(self, query: str, execution_time: float, threshold: float = 5.0) -> None:
        """记录慢查询"""
        if execution_time > threshold:
            self.logger.warning(
                f"Slow query detected: {query[:100]}... "
                f"Time: {execution_time:.2f}s "
                f"Threshold: {threshold:.2f}s"
            )
```

### 5.2 性能监控

```python
from typing import Any, Optional
import time
from collections import defaultdict
from superset.common.query_logger import QueryLogger

class PerformanceMonitor:
    """性能监控"""

    def __init__(self):
        self.query_logger = QueryLogger()
        self.query_stats = defaultdict(list)

    def monitor_query(self, query: str, execute_func) -> Any:
        """监控查询执行"""
        start_time = time.time()

        try:
            result = execute_func()
            execution_time = time.time() - start_time

            # 记录统计信息
            self.query_stats['execution_times'].append(execution_time)
            self.query_stats['query_count'] += 1

            # 记录日志
            row_count = len(result) if hasattr(result, '__len__') else 0
            self.query_logger.log_query(query, execution_time, row_count)

            # 检查慢查询
            self.query_logger.log_slow_query(query, execution_time)

            return result
        except Exception as e:
            execution_time = time.time() - start_time
            self.query_logger.log_error(query, e)
            raise

    def get_stats(self) -> dict[str, Any]:
        """获取统计信息"""
        execution_times = self.query_stats.get('execution_times', [])

        if not execution_times:
            return {}

        return {
            'query_count': self.query_stats.get('query_count', 0),
            'avg_execution_time': sum(execution_times) / len(execution_times),
            'min_execution_time': min(execution_times),
            'max_execution_time': max(execution_times),
            'total_execution_time': sum(execution_times),
        }

    def reset_stats(self) -> None:
        """重置统计信息"""
        self.query_stats.clear()
```

### 5.3 错误跟踪

```python
from typing import Any, Optional
import logging
from datetime import datetime

logger = logging.getLogger(__name__)

class ErrorTracker:
    """错误跟踪"""

    def __init__(self):
        self.logger = logger
        self.errors = []

    def track_error(self, query: str, error: Exception, context: Optional[dict[str, Any]] = None) -> None:
        """跟踪错误"""
        error_info = {
            'query': query,
            'error_type': type(error).__name__,
            'error_message': str(error),
            'timestamp': datetime.utcnow().isoformat(),
            'context': context or {},
        }

        self.errors.append(error_info)

        # 记录日志
        self.logger.error(
            f"Error tracked: {error_info['error_type']}: {error_info['error_message']}"
        )

    def get_errors(self, limit: Optional[int] = None) -> list[dict[str, Any]]:
        """获取错误列表"""
        if limit:
            return self.errors[-limit:]
        return self.errors

    def get_error_summary(self) -> dict[str, Any]:
        """获取错误摘要"""
        if not self.errors:
            return {}

        error_types = {}
        for error in self.errors:
            error_type = error['error_type']
            error_types[error_type] = error_types.get(error_type, 0) + 1

        return {
            'total_errors': len(self.errors),
            'error_types': error_types,
            'most_common_error': max(error_types, key=error_types.get) if error_types else None,
        }

    def clear_errors(self) -> None:
        """清空错误记录"""
        self.errors.clear()
```

## 6. 完整查询流程

### 6.1 查询执行流程

```python
from typing import Any, Optional
from superset.common.query_object import QueryObject
from superset.models.core import SqlaTable
from superset.common.query_processor import (
    QueryBuilder,
    SQLGenerator,
    QueryExecutor,
    ResultProcessor,
    QueryOptimizer,
    QueryCacheManager,
    QueryAnalyzer,
    PerformanceMonitor,
)

class QueryEngine:
    """查询引擎"""

    def __init__(self, datasource: SqlaTable):
        self.datasource = datasource
        self.optimizer = QueryOptimizer(datasource)
        self.cache_manager = QueryCacheManager()
        self.analyzer = QueryAnalyzer(datasource)
        self.performance_monitor = PerformanceMonitor()

    def execute(self, query_object: QueryObject) -> dict[str, Any]:
        """执行查询"""
        # 检查缓存
        cached_result = self.cache_manager.get_cached_result(query_object, self.datasource)
        if cached_result:
            return cached_result

        # 分析查询
        analysis = self.analyzer.analyze(query_object)
        if analysis['recommendations']:
            logger.info(f"Query recommendations: {analysis['recommendations']}")

        # 优化查询
        optimized_query = self.optimizer.optimize(query_object)

        # 生成 SQL
        sql_generator = SQLGenerator(self.datasource)
        sql = sql_generator.generate_sql(optimized_query)

        # 执行查询
        executor = QueryExecutor(self.datasource.database)
        df = self.performance_monitor.monitor_query(sql, lambda: executor.execute(sql))

        # 处理结果
        result_processor = ResultProcessor(df)
        if optimized_query.post_processing:
            df = result_processor.apply_post_processing(optimized_query.post_processing)

        result = result_processor.process()

        # 缓存结果
        self.cache_manager.cache_result(
            query_object,
            self.datasource,
            result,
            timeout=3600,
        )

        return result
```

## 7. 下一步

阅读完本文档后，建议继续学习：

1. [核心模块](./06-核心模块.md) - 学习核心业务模块
2. [请求流程](./14-请求流程.md) - 了解请求处理流程
3. [缓存机制](./16-缓存机制.md) - 学习缓存策略
