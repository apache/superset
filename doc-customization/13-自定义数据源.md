# Apache Superset 自定义数据源

## 1. 自定义数据源概述

### 1.1 数据源类型

Apache Superset 支持多种数据源类型：

| 数据源类型 | 描述 | 示例 |
|-----------|------|------|
| **关系型数据库** | SQL 数据库 | PostgreSQL, MySQL, SQLite |
| **NoSQL 数据库** | 文档数据库 | MongoDB, Cassandra |
| **大数据平台** | 分布式数据存储 | Hive, Spark, Presto |
| **云数据仓库** | 云端数据仓库 | Snowflake, BigQuery, Redshift |
| **数据文件** | 文件格式 | CSV, Excel, JSON |
| **API 数据源** | RESTful API | 自定义 API |
| **流数据源** | 实时数据流 | Kafka, Kinesis |

### 1.2 自定义数据源架构

```
自定义数据源架构
├── 数据库引擎规范 (Engine Spec)
│   ├── 连接管理
│   ├── SQL 生成
│   ├── 类型映射
│   └── 元数据获取
├── 数据连接器 (Data Connector)
│   ├── 连接池管理
│   ├── 查询执行
│   ├── 结果处理
│   └── 错误处理
├── 数据转换器 (Data Transformer)
│   ├── 数据解析
│   ├── 数据清洗
│   ├── 数据聚合
│   └── 数据格式化
└── 缓存管理器 (Cache Manager)
    ├── 查询缓存
    ├── 元数据缓存
    └── 缓存失效
```

## 2. 数据库引擎规范

### 2.1 基础引擎规范

[db_engine_specs/base.py](../superset/db_engine_specs/base.py) 定义了基础引擎规范：

```python
from sqlalchemy.engine.url import URL
from typing import Any, Optional, List, Type
from datetime import datetime
import pandas as pd

class BaseEngineSpec:
    """基础数据库引擎规范"""

    engine = None
    engine_name = None
    default_driver = None

    @classmethod
    def get_dbapi_uri(cls, connection_params: dict[str, Any]) -> str:
        """获取数据库连接 URI"""
        raise NotImplementedError

    @classmethod
    def get_sqla_column_type(cls, sqla_type: Any) -> Any:
        """获取 SQLAlchemy 列类型"""
        raise NotImplementedError

    @classmethod
    def convert_dttm(cls, dttm: datetime) -> str:
        """转换日期时间为数据库格式"""
        raise NotImplementedError

    @classmethod
    def epoch_to_dttm(cls) -> str:
        """获取 epoch 到日期时间的转换 SQL"""
        raise NotImplementedError

    @classmethod
    def epoch_ms_to_dttm(cls) -> str:
        """获取 epoch 毫秒到日期时间的转换 SQL"""
        raise NotImplementedError

    @classmethod
    def get_metrics(cls, metrics: list[Any]) -> list[Any]:
        """获取指标"""
        return metrics

    @classmethod
    def get_groupby(cls, groupby: list[Any]) -> list[Any]:
        """获取分组"""
        return groupby

    @classmethod
    def get_column_spec(cls, column: Any) -> dict[str, Any]:
        """获取列规范"""
        return {
            "type": column.type,
            "label": column.column_name,
            "is_dttm": column.is_temporal,
        }

    @classmethod
    def get_table_names(cls, database: Any, schema: Optional[str] = None) -> list[str]:
        """获取表名列表"""
        raise NotImplementedError

    @classmethod
    def get_columns(cls, database: Any, table_name: str, schema: Optional[str] = None) -> list[dict[str, Any]]:
        """获取表的列信息"""
        raise NotImplementedError

    @classmethod
    def get_sql(cls, query_object: Any, datasource: Any) -> str:
        """生成 SQL"""
        raise NotImplementedError

    @classmethod
    def execute_sql(cls, database: Any, sql: str) -> pd.DataFrame:
        """执行 SQL"""
        engine = database.get_sqla_engine()
        with engine.connect() as connection:
            df = pd.read_sql_query(sql, connection)
        return df

    @classmethod
    def get_engine_spec_by_uri(cls, uri: str) -> Type['BaseEngineSpec']:
        """根据 URI 获取引擎规范"""
        from superset.db_engine_specs import get_engine_spec_by_name

        engine_name = URL.create(uri).drivername
        return get_engine_spec_by_name(engine_name)
```

### 2.2 创建自定义引擎规范

```python
# superset/db_engine_specs/mydb.py
from superset.db_engine_specs.base import BaseEngineSpec
from sqlalchemy.engine.url import URL
from typing import Any, Optional, List
from datetime import datetime
import pandas as pd
from sqlalchemy import Integer, String, DateTime, Float
from sqlalchemy import inspect

class MyDatabaseEngineSpec(BaseEngineSpec):
    """自定义数据库引擎规范"""

    engine = "mydb"
    engine_name = "My Database"
    default_driver = "mydb-driver"

    @classmethod
    def get_dbapi_uri(cls, connection_params: dict[str, Any]) -> str:
        """获取数据库连接 URI"""
        return str(
            URL.create(
                drivername=cls.engine,
                username=connection_params.get("username"),
                password=connection_params.get("password"),
                host=connection_params.get("host"),
                port=connection_params.get("port"),
                database=connection_params.get("database"),
            )
        )

    @classmethod
    def get_sqla_column_type(cls, sqla_type: Any) -> Any:
        """获取 SQLAlchemy 列类型"""
        type_mapping = {
            "INTEGER": Integer,
            "BIGINT": Integer,
            "VARCHAR": String,
            "TEXT": String,
            "TIMESTAMP": DateTime,
            "DATE": DateTime,
            "FLOAT": Float,
            "DOUBLE": Float,
            "DECIMAL": Float,
        }

        return type_mapping.get(sqla_type, String)

    @classmethod
    def convert_dttm(cls, dttm: datetime) -> str:
        """转换日期时间为数据库格式"""
        return f"'{dttm.strftime('%Y-%m-%d %H:%M:%S')}'"

    @classmethod
    def epoch_to_dttm(cls) -> str:
        """获取 epoch 到日期时间的转换 SQL"""
        return "FROM_UNIXTIME({col})"

    @classmethod
    def epoch_ms_to_dttm(cls) -> str:
        """获取 epoch 毫秒到日期时间的转换 SQL"""
        return "FROM_UNIXTIME({col} / 1000)"

    @classmethod
    def get_table_names(cls, database: Any, schema: Optional[str] = None) -> List[str]:
        """获取表名列表"""
        engine = database.get_sqla_engine()
        inspector = inspect(engine)

        if schema:
            return inspector.get_table_names(schema=schema)
        else:
            return inspector.get_table_names()

    @classmethod
    def get_columns(cls, database: Any, table_name: str, schema: Optional[str] = None) -> List[dict[str, Any]]:
        """获取表的列信息"""
        engine = database.get_sqla_engine()
        inspector = inspect(engine)

        columns = inspector.get_columns(table_name, schema=schema)

        return [
            {
                "name": col["name"],
                "type": str(col["type"]),
                "is_dttm": cls.is_temporal_type(col["type"]),
            }
            for col in columns
        ]

    @classmethod
    def is_temporal_type(cls, col_type: Any) -> bool:
        """检查是否是时间类型"""
        temporal_types = ["DATE", "TIMESTAMP", "TIME", "DATETIME"]
        return any(t in str(col_type).upper() for t in temporal_types)

    @classmethod
    def get_sql(cls, query_object: Any, datasource: Any) -> str:
        """生成 SQL"""
        from sqlalchemy import select, func

        table = datasource.get_sqla_table()
        query = select()

        # 添加指标
        for metric in query_object.metrics:
            if hasattr(metric, 'get_sqla_expression'):
                query = query.add_columns(metric.get_sqla_expression())
            else:
                query = query.add_columns(func.sum(table.c[metric]))

        # 添加分组
        if query_object.groupby:
            for group in query_object.groupby:
                query = query.group_by(table.c[group])

        # 添加时间范围
        if query_object.from_dttm and query_object.to_dttm:
            time_col = datasource.main_dttm_col
            query = query.where(
                table.c[time_col] >= query_object.from_dttm,
                table.c[time_col] <= query_object.to_dttm,
            )

        # 添加过滤
        for filter_ in query_object.filters:
            query = query.where(filter_)

        # 添加排序
        if query_object.orderby:
            for order in query_object.orderby:
                query = query.order_by(table.c[order])

        # 添加限制
        if query_object.row_limit:
            query = query.limit(query_object.row_limit)

        return str(query)

    @classmethod
    def execute_sql(cls, database: Any, sql: str) -> pd.DataFrame:
        """执行 SQL"""
        engine = database.get_sqla_engine()
        with engine.connect() as connection:
            df = pd.read_sql_query(sql, connection)
        return df
```

### 2.3 注册自定义引擎

```python
# superset/db_engine_specs/__init__.py
from superset.db_engine_specs.mydb import MyDatabaseEngineSpec
from superset.db_engine_specs.base import BaseEngineSpec

# 注册引擎
ENGINE_SPECS: dict[str, Type[BaseEngineSpec]] = {
    "mydb": MyDatabaseEngineSpec,
    # 其他引擎...
}

def get_engine_spec_by_name(name: str) -> Type[BaseEngineSpec]:
    """根据名称获取引擎规范"""
    return ENGINE_SPECS.get(name, BaseEngineSpec)
```

## 3. API 数据源

### 3.1 API 数据源引擎规范

```python
# superset/db_engine_specs/api.py
from superset.db_engine_specs.base import BaseEngineSpec
from typing import Any, Optional, List
import pandas as pd
import requests

class APIEngineSpec(BaseEngineSpec):
    """API 数据源引擎规范"""

    engine = "api"
    engine_name = "API"
    default_driver = "requests"

    @classmethod
    def get_dbapi_uri(cls, connection_params: dict[str, Any]) -> str:
        """获取 API 连接 URI"""
        base_url = connection_params.get("base_url")
        return base_url

    @classmethod
    def get_sqla_column_type(cls, sqla_type: Any) -> Any:
        """获取 SQLAlchemy 列类型"""
        from sqlalchemy import String, Integer, Float, DateTime

        type_mapping = {
            "string": String,
            "integer": Integer,
            "float": Float,
            "datetime": DateTime,
        }

        return type_mapping.get(sqla_type, String)

    @classmethod
    def convert_dttm(cls, dttm: datetime) -> str:
        """转换日期时间为 API 格式"""
        return dttm.isoformat()

    @classmethod
    def epoch_to_dttm(cls) -> str:
        """获取 epoch 到日期时间的转换 SQL"""
        return "FROM_UNIXTIME({col})"

    @classmethod
    def get_table_names(cls, database: Any, schema: Optional[str] = None) -> List[str]:
        """获取 API 端点列表"""
        base_url = database.sqlalchemy_uri
        response = requests.get(f"{base_url}/endpoints")

        if response.status_code == 200:
            return response.json().get("endpoints", [])

        return []

    @classmethod
    def get_columns(cls, database: Any, table_name: str, schema: Optional[str] = None) -> List[dict[str, Any]]:
        """获取 API 端点的字段信息"""
        base_url = database.sqlalchemy_uri
        response = requests.get(f"{base_url}/endpoints/{table_name}/schema")

        if response.status_code == 200:
            schema = response.json()
            return [
                {
                    "name": field["name"],
                    "type": field["type"],
                    "is_dttm": field["type"] in ["datetime", "timestamp"],
                }
                for field in schema.get("fields", [])
            ]

        return []

    @classmethod
    def get_sql(cls, query_object: Any, datasource: Any) -> str:
        """生成 API 查询参数"""
        params = {}

        # 添加指标
        if query_object.metrics:
            params["metrics"] = query_object.metrics

        # 添加分组
        if query_object.groupby:
            params["groupby"] = query_object.groupby

        # 添加时间范围
        if query_object.from_dttm and query_object.to_dttm:
            params["from"] = query_object.from_dttm
            params["to"] = query_object.to_dttm

        # 添加过滤
        if query_object.filters:
            params["filters"] = query_object.filters

        # 添加排序
        if query_object.orderby:
            params["orderby"] = query_object.orderby

        # 添加限制
        if query_object.row_limit:
            params["limit"] = query_object.row_limit

        return params

    @classmethod
    def execute_sql(cls, database: Any, sql: str) -> pd.DataFrame:
        """执行 API 查询"""
        base_url = database.sqlalchemy_uri
        endpoint = sql.get("endpoint", "data")
        params = sql

        response = requests.get(f"{base_url}/{endpoint}", params=params)

        if response.status_code == 200:
            data = response.json()
            return pd.DataFrame(data.get("records", []))

        return pd.DataFrame()
```

### 3.2 API 数据源配置

```python
from superset.models.core import Database
from superset.extensions import db

# 创建 API 数据源
database = Database(
    database_name='My API',
    sqlalchemy_uri='https://api.example.com',
    verbose_name='我的 API 数据源',
    allow_ctas=False,
    allow_cvas=False,
)

db.session.add(database)
db.session.commit()
```

## 4. 文件数据源

### 4.1 CSV 文件数据源

```python
# superset/db_engine_specs/csv.py
from superset.db_engine_specs.base import BaseEngineSpec
from typing import Any, Optional, List
import pandas as pd
import os

class CSVEngineSpec(BaseEngineSpec):
    """CSV 文件数据源引擎规范"""

    engine = "csv"
    engine_name = "CSV"
    default_driver = "pandas"

    @classmethod
    def get_dbapi_uri(cls, connection_params: dict[str, Any]) -> str:
        """获取 CSV 文件路径"""
        file_path = connection_params.get("file_path")
        return file_path

    @classmethod
    def get_sqla_column_type(cls, sqla_type: Any) -> Any:
        """获取 SQLAlchemy 列类型"""
        from sqlalchemy import String, Integer, Float, DateTime

        type_mapping = {
            "object": String,
            "int64": Integer,
            "float64": Float,
            "datetime64[ns]": DateTime,
        }

        return type_mapping.get(sqla_type, String)

    @classmethod
    def convert_dttm(cls, dttm: datetime) -> str:
        """转换日期时间为 CSV 格式"""
        return dttm.strftime('%Y-%m-%d %H:%M:%S')

    @classmethod
    def epoch_to_dttm(cls) -> str:
        """获取 epoch 到日期时间的转换 SQL"""
        return "FROM_UNIXTIME({col})"

    @classmethod
    def get_table_names(cls, database: Any, schema: Optional[str] = None) -> List[str]:
        """获取 CSV 文件列表"""
        file_path = database.sqlalchemy_uri
        directory = os.path.dirname(file_path)

        if os.path.exists(directory):
            return [
                f for f in os.listdir(directory)
                if f.endswith('.csv')
            ]

        return []

    @classmethod
    def get_columns(cls, database: Any, table_name: str, schema: Optional[str] = None) -> List[dict[str, Any]]:
        """获取 CSV 文件的列信息"""
        file_path = database.sqlalchemy_uri
        df = pd.read_csv(file_path, nrows=1)

        return [
            {
                "name": col,
                "type": str(df[col].dtype),
                "is_dttm": pd.api.types.is_datetime64_any_dtype(df[col]),
            }
            for col in df.columns
        ]

    @classmethod
    def get_sql(cls, query_object: Any, datasource: Any) -> str:
        """生成 CSV 查询参数"""
        params = {}

        # 添加列选择
        if query_object.columns:
            params["columns"] = query_object.columns

        # 添加过滤
        if query_object.filters:
            params["filters"] = query_object.filters

        # 添加排序
        if query_object.orderby:
            params["orderby"] = query_object.orderby

        # 添加限制
        if query_object.row_limit:
            params["limit"] = query_object.row_limit

        return params

    @classmethod
    def execute_sql(cls, database: Any, sql: str) -> pd.DataFrame:
        """执行 CSV 查询"""
        file_path = database.sqlalchemy_uri
        params = sql

        # 读取 CSV 文件
        df = pd.read_csv(file_path)

        # 应用列选择
        if "columns" in params:
            df = df[params["columns"]]

        # 应用过滤
        if "filters" in params:
            for filter_ in params["filters"]:
                column = filter_["column"]
                operator = filter_["operator"]
                value = filter_["value"]

                if operator == "==":
                    df = df[df[column] == value]
                elif operator == "!=":
                    df = df[df[column] != value]
                elif operator == ">":
                    df = df[df[column] > value]
                elif operator == "<":
                    df = df[df[column] < value]
                elif operator == ">=":
                    df = df[df[column] >= value]
                elif operator == "<=":
                    df = df[df[column] <= value]
                elif operator == "in":
                    df = df[df[column].isin(value)]
                elif operator == "not in":
                    df = df[~df[column].isin(value)]

        # 应用排序
        if "orderby" in params:
            df = df.sort_values(by=params["orderby"])

        # 应用限制
        if "limit" in params:
            df = df.head(params["limit"])

        return df
```

### 4.2 CSV 文件数据源配置

```python
from superset.models.core import Database
from superset.extensions import db

# 创建 CSV 文件数据源
database = Database(
    database_name='My CSV Data',
    sqlalchemy_uri='/path/to/data.csv',
    verbose_name='我的 CSV 数据源',
    allow_ctas=False,
    allow_cvas=False,
)

db.session.add(database)
db.session.commit()
```

## 5. 数据连接器

### 5.1 基础数据连接器

```python
from typing import Any, Optional
from sqlalchemy import create_engine, Engine
from sqlalchemy.pool import QueuePool
from superset.models.core import Database

class DatabaseConnector:
    """数据库连接器"""

    def __init__(self, database: Database):
        self.database = database
        self._engine: Optional[Engine] = None

    def get_engine(self) -> Engine:
        """获取数据库引擎"""
        if self._engine is None:
            self._engine = self._create_engine()
        return self._engine

    def _create_engine(self) -> Engine:
        """创建数据库引擎"""
        return create_engine(
            self.database.sqlalchemy_uri,
            poolclass=QueuePool,
            pool_size=5,
            max_overflow=10,
            pool_pre_ping=True,
            pool_recycle=3600,
        )

    def test_connection(self) -> bool:
        """测试连接"""
        try:
            engine = self.get_engine()
            with engine.connect() as connection:
                connection.execute("SELECT 1")
            return True
        except Exception:
            return False

    def close(self) -> None:
        """关闭连接"""
        if self._engine:
            self._engine.dispose()
            self._engine = None
```

### 5.2 连接池管理

```python
from typing import Any, Optional
from sqlalchemy import create_engine, Engine
from sqlalchemy.pool import QueuePool
from superset.models.core import Database

class ConnectionPoolManager:
    """连接池管理器"""

    def __init__(self, database: Database):
        self.database = database
        self._engine: Optional[Engine] = None

    def get_engine(self) -> Engine:
        """获取数据库引擎"""
        if self._engine is None:
            self._engine = self._create_engine()
        return self._engine

    def _create_engine(self) -> Engine:
        """创建数据库引擎"""
        return create_engine(
            self.database.sqlalchemy_uri,
            poolclass=QueuePool,
            pool_size=5,
            max_overflow=10,
            pool_pre_ping=True,
            pool_recycle=3600,
            pool_timeout=30,
            pool_reset_on_return='commit',
        )

    def get_pool_status(self) -> dict[str, Any]:
        """获取连接池状态"""
        if not self._engine:
            return {}

        pool = self._engine.pool
        return {
            "size": pool.size(),
            "checked_in": pool.checkedin(),
            "checked_out": pool.checkedout(),
            "overflow": pool.overflow(),
            "invalid": pool.invalid(),
        }

    def close(self) -> None:
        """关闭连接池"""
        if self._engine:
            self._engine.dispose()
            self._engine = None
```

## 6. 数据转换器

### 6.1 基础数据转换器

```python
from typing import Any, Optional
import pandas as pd

class DataTransformer:
    """数据转换器"""

    def __init__(self, df: pd.DataFrame):
        self.df = df

    def transform(self, transformations: list[dict[str, Any]]) -> pd.DataFrame:
        """应用数据转换"""
        for transformation in transformations:
            self.df = self.apply_transformation(transformation)

        return self.df

    def apply_transformation(self, transformation: dict[str, Any]) -> pd.DataFrame:
        """应用单个转换"""
        operation = transformation.get("operation")

        if operation == "rename":
            return self.rename_columns(transformation)
        elif operation == "filter":
            return self.filter_rows(transformation)
        elif operation == "aggregate":
            return self.aggregate(transformation)
        elif operation == "pivot":
            return self.pivot(transformation)
        elif operation == "sort":
            return self.sort(transformation)
        else:
            return self.df

    def rename_columns(self, transformation: dict[str, Any]) -> pd.DataFrame:
        """重命名列"""
        mapping = transformation.get("mapping", {})
        return self.df.rename(columns=mapping)

    def filter_rows(self, transformation: dict[str, Any]) -> pd.DataFrame:
        """过滤行"""
        column = transformation.get("column")
        operator = transformation.get("operator")
        value = transformation.get("value")

        if operator == "==":
            return self.df[self.df[column] == value]
        elif operator == "!=":
            return self.df[self.df[column] != value]
        elif operator == ">":
            return self.df[self.df[column] > value]
        elif operator == "<":
            return self.df[self.df[column] < value]
        elif operator == ">=":
            return self.df[self.df[column] >= value]
        elif operator == "<=":
            return self.df[self.df[column] <= value]
        elif operator == "in":
            return self.df[self.df[column].isin(value)]
        elif operation == "not in":
            return self.df[~self.df[column].isin(value)]
        else:
            return self.df

    def aggregate(self, transformation: dict[str, Any]) -> pd.DataFrame:
        """聚合数据"""
        groupby = transformation.get("groupby", [])
        metrics = transformation.get("metrics", [])

        if not metrics:
            return self.df

        agg_dict = {}
        for metric in metrics:
            column = metric.get("column")
            func = metric.get("function", "sum")

            if column not in agg_dict:
                agg_dict[column] = []

            agg_dict[column].append(func)

        if groupby:
            return self.df.groupby(groupby).agg(agg_dict).reset_index()
        else:
            return self.df.agg(agg_dict)

    def pivot(self, transformation: dict[str, Any]) -> pd.DataFrame:
        """透视数据"""
        index = transformation.get("index")
        columns = transformation.get("columns")
        values = transformation.get("values")

        return self.df.pivot_table(
            index=index,
            columns=columns,
            values=values,
            aggfunc='sum',
        ).reset_index()

    def sort(self, transformation: dict[str, Any]) -> pd.DataFrame:
        """排序数据"""
        columns = transformation.get("columns", [])
        ascending = transformation.get("ascending", True)

        return self.df.sort_values(by=columns, ascending=ascending)
```

## 7. 缓存管理器

### 7.1 查询缓存管理器

```python
from typing import Any, Optional
import hashlib
import json
from superset.extensions import cache_manager

class QueryCacheManager:
    """查询缓存管理器"""

    def __init__(self):
        self.cache = cache_manager

    def get_cache_key(self, query: str, datasource_id: int) -> str:
        """获取缓存键"""
        cache_data = {
            'datasource_id': datasource_id,
            'query': query,
        }

        cache_string = json.dumps(cache_data, sort_keys=True)
        cache_key = hashlib.md5(cache_string.encode()).hexdigest()

        return f"query:{datasource_id}:{cache_key}"

    def get_cached_result(self, query: str, datasource_id: int) -> Optional[pd.DataFrame]:
        """获取缓存结果"""
        cache_key = self.get_cache_key(query, datasource_id)
        result = self.cache.get(cache_key)

        if result:
            import pandas as pd
            return pd.DataFrame(result)

        return None

    def cache_result(self, query: str, datasource_id: int, df: pd.DataFrame, timeout: int = 3600) -> None:
        """缓存结果"""
        cache_key = self.get_cache_key(query, datasource_id)
        self.cache.set(cache_key, df.to_dict('records'), timeout=timeout)

    def invalidate_cache(self, query: str, datasource_id: int) -> None:
        """使缓存失效"""
        cache_key = self.get_cache_key(query, datasource_id)
        self.cache.delete(cache_key)

    def clear_all_cache(self) -> None:
        """清空所有缓存"""
        self.cache.clear()
```

## 8. 下一步

阅读完本文档后，建议继续学习：

1. [插件系统](./11-插件系统.md) - 学习插件系统架构
2. [自定义图表](./12-自定义图表.md) - 学习如何创建自定义图表
3. [部署配置](./15-部署配置.md) - 学习部署和配置
